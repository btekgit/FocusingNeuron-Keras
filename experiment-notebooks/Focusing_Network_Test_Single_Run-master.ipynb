{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n * Focusing Neuron Test Case\\n *\\n * Code Authors:  F. Boray Tek and \\n * İlker Çam contributed to an earlier version\\n *\\n * All rights reserved.\\n *\\n * For details, see the paper:\\n * \\n *  \\n * \\n *\\n * Permission to use, copy, modify, and distribute this software and\\n * its documentation for educational, research, and non-commercial\\n * purposes, without fee and without a signed licensing agreement, is\\n * hereby granted, provided that the above copyright notice and this\\n * paragraph appear in all copies modifications, and distributions.\\n *\\n * NOTE: THIS WORK IS PATENT PENDING!\\n * Patent rights are owned by F. Boray Tek, İlker Çam, Işık University\\n * Any commercial use or any redistribution of this software\\n * requires a license from one of the above mentioned establishments.\\n *\\n * For further details, contact F. Boray Tek (boraytek@gmail.com).\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#from __future__ import unicode_literals\n",
    "\n",
    "'''\n",
    " * Focusing Neuron Test Case\n",
    " *\n",
    " * Code Authors:  F. Boray Tek and \n",
    " * İlker Çam contributed to an earlier version\n",
    " *\n",
    " * All rights reserved.\n",
    " *\n",
    " * For details, see the paper:\n",
    " * \n",
    " *  \n",
    " * \n",
    " *\n",
    " * Permission to use, copy, modify, and distribute this software and\n",
    " * its documentation for educational, research, and non-commercial\n",
    " * purposes, without fee and without a signed licensing agreement, is\n",
    " * hereby granted, provided that the above copyright notice and this\n",
    " * paragraph appear in all copies modifications, and distributions.\n",
    " *\n",
    " * NOTE: THIS WORK IS PATENT PENDING!\n",
    " * Patent rights are owned by F. Boray Tek, İlker Çam, Işık University\n",
    " * Any commercial use or any redistribution of this software\n",
    " * requires a license from one of the above mentioned establishments.\n",
    " *\n",
    " * For further details, contact F. Boray Tek (boraytek@gmail.com).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['THEANO_FLAGS']='device=cuda0,floatX=float32,preallocate=0.1'\n",
    "os.environ['MKL_THREADING_LAYER']='GNU'\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import lasagne\n",
    "import theano\n",
    "theano.config.exception_verbosity = 'high'\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "from focusing import FocusedLayer1D\n",
    "from collections import OrderedDict\n",
    "from lasagne.updates import get_or_compute_grads, apply_momentum, sgd\n",
    "from lasagne.updates import momentum, adam, adadelta\n",
    "from lasagne.regularization import regularize_layer_params_weighted, l2, l1\n",
    "from lasagne.regularization import regularize_layer_params\n",
    "\n",
    "\n",
    "\n",
    "from lasagne_utils import sgdWithLrsClip,categorical_focal_loss,\\\n",
    "get_shared_by_pattern,sgdWithLrs,iterate_minibatches,set_params_value,\\\n",
    "print_param_stats, get_params_values_wkey,sgdWithWeightSupress\n",
    "\n",
    "from data_utils import load_dataset_mnist, load_dataset_mnist_cluttered,\\\n",
    "load_dataset_fashion, load_dataset_cifar10\n",
    "\n",
    "import pandas as pd\n",
    "#pandas.DataFrame(data=None, index=None, columns=None, dtype=None, copy=False)[source]¶\n",
    "from data_utils import load_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "font = {'family' : 'arial',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 22}\n",
    "\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dense_model(input_shape,input_var, fparams):\n",
    "    # Initers, Layers\n",
    "    nhidden = fparams['nHIDDEN']\n",
    "    nclasses = fparams['nCLASSES']\n",
    "    batchnorm = fparams['USEBATCHNORM']\n",
    "    use_penalty = fparams['USE_PENALTY']\n",
    "    ini = lasagne.init.GlorotUniform()\n",
    "    \n",
    "    nonlin = lasagne.nonlinearities.rectify\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    \n",
    "    lin = lasagne.nonlinearities.linear\n",
    "    \n",
    "    # Input Layer\n",
    "    l_in = lasagne.layers.InputLayer(shape=input_shape, input_var=input_var)\n",
    "    \n",
    "    # Denses\n",
    "    l_dense1 = lasagne.layers.DenseLayer(\n",
    "            l_in, num_units=nhidden, \n",
    "            nonlinearity=lin, \n",
    "            W=ini, name=\"dense1\")\n",
    "    \n",
    "    \n",
    "    if batchnorm:\n",
    "    # if you close BATCHNORM weights get LARGE\n",
    "        l_bn = lasagne.layers.NonlinearityLayer(\n",
    "                lasagne.layers.BatchNormLayer(l_dense1), nonlinearity=nonlin)\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        l_bn = lasagne.layers.NonlinearityLayer(l_dense1, nonlinearity=nonlin)\n",
    "    #l_dense2 = lasagne.layers.DenseLayer(l_dense1, num_units=4, nonlinearity=lasagne.nonlinearities.tanh, W=ini, name='dense2')\n",
    "    \n",
    "    #l_drop1 = lasagne.layers.dropout(l_bn, p=0.1)\n",
    "    \n",
    "    # Output Layer\n",
    "    l_out = lasagne.layers.DenseLayer(l_bn, num_units=nclasses, nonlinearity=softmax, W=ini, name='output')\n",
    "    \n",
    "    \n",
    "    penalty = l2(l_dense1.W)*1e-5+l2(l_out.W)*1e-5\n",
    "    if not use_penalty:\n",
    "        penalty = penalty*0\n",
    "    \n",
    "    #penalty = penalty*0\n",
    "    #penalty = (l2(l_dense1.W)*1e-30)#(l2(l_dense1.W)*1e-3)+(l1(l_dense1.W)*1e-6) +(l2(l_out.W)*1e-3)\n",
    "    \n",
    "    return l_out, penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_focused_model(input_shape,input_var, fparams):\n",
    "    # Initers, Layers\n",
    "    \n",
    "    nhidden = fparams['nHIDDEN']\n",
    "    nclasses = fparams['nCLASSES']\n",
    "    batchnorm = fparams['USEBATCHNORM']\n",
    "    use_penalty = fparams['USE_PENALTY']\n",
    "    \n",
    "    ini = lasagne.init.GlorotUniform()\n",
    "    nonlin = lasagne.nonlinearities.rectify\n",
    "    linear = lasagne.nonlinearities.linear\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "\n",
    "    \n",
    "    # Input Layer\n",
    "    l_in = lasagne.layers.InputLayer(shape=input_shape, input_var=input_var)\n",
    "    \n",
    "    l_focus1 = FocusedLayer1D(l_in, num_units=nhidden, \n",
    "                              nonlinearity=linear, name='focus1',\n",
    "                              trainMus=fparams['trainMus'],\n",
    "                              trainSis=fparams['trainSis'],\n",
    "                              initMu=fparams['initMu'], \n",
    "                              W=ini, withWeights=fparams['withWeights'], \n",
    "                              bias=lasagne.init.Constant(0.0), \n",
    "                              initSigma=fparams['initSigma'], \n",
    "                              trainWs=fparams['trainWs'])\n",
    "                                               \n",
    "    \n",
    "    if batchnorm:\n",
    "    # if you close BATCHNORM weights get LARGE\n",
    "        l_bn = lasagne.layers.NonlinearityLayer(\n",
    "                lasagne.layers.BatchNormLayer(l_focus1), nonlinearity=nonlin)\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        l_bn = lasagne.layers.NonlinearityLayer(l_focus1, nonlinearity=nonlin)\n",
    " \n",
    "    #l_drop1 = lasagne.layers.dropout(l_bn, p=0.1)\n",
    "    \n",
    "    # Output\n",
    "    l_out = lasagne.layers.DenseLayer(l_bn, num_units=nclasses, \n",
    "                                      nonlinearity=softmax, W=ini, name='output')\n",
    "    \n",
    "    penalty = l2(l_out.W)*np.float32(1e-5)\n",
    "    # if focusing neuron has weights. add penalty to them as well. A 10th factors less because focus function already suppresses weghts\n",
    "    if fparams['withWeights']:\n",
    "        penalty += l2(l_focus1.W)*np.float32(1e-5)\n",
    "    \n",
    "    if not use_penalty:\n",
    "        penalty = penalty*0\n",
    "    \n",
    "    \n",
    "    return l_out, penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile train and eval functions\n",
    "def build_functions(using_model, X, y, fparams, penalty):\n",
    "    \n",
    "\n",
    "    # training output\n",
    "    output_train = lasagne.layers.get_output(using_model, X, deterministic=False)\n",
    "\n",
    "    # evaluation output. Also includes output of transform for plotting\n",
    "    output_eval = lasagne.layers.get_output(using_model, X, deterministic=True)\n",
    "    \n",
    "    cost_train = T.mean(lasagne.objectives.categorical_crossentropy(output_train, y)) + penalty # Regularization\n",
    "    cost_eval = T.mean(lasagne.objectives.categorical_crossentropy(output_eval, y))\n",
    "  \n",
    "    network_params = lasagne.layers.get_all_params(using_model, trainable=True)\n",
    "    print(\"Params\",network_params)\n",
    "    n_params =  lasagne.layers.count_params(using_model)\n",
    "    print(\"Param count:\",n_params)\n",
    "    \n",
    "    LR_rate = theano.shared(np.float32(fparams['LEARNING_RATE']),name='lr_all')\n",
    "    LR_MU = theano.shared(np.float32(fparams['LR_MU']),name='lr_mu')\n",
    "    LR_SI = theano.shared(np.float32(fparams['LR_SI']),name='lr_si')\n",
    "    LR_FW = theano.shared(np.float32(fparams['LR_FW']),name='lr_fw')\n",
    "    LR_params = [LR_rate, LR_MU, LR_SI, LR_FW]\n",
    "        \n",
    "    updates = sgdWithLrsClip(cost_train, network_params, learning_rate=LR_rate, \n",
    "                         mu_lr=LR_MU, si_lr=LR_SI, \n",
    "                         focused_w_lr=LR_FW, momentum=fparams['MOMENTUM'])\n",
    "    \n",
    "    test_acc = T.mean(T.eq(T.argmax(output_eval, axis=1), y), dtype=theano.config.floatX)\n",
    "\n",
    "    trainf = theano.function([X, y], [cost_train, output_train, penalty], updates=updates, allow_input_downcast=True, name='train')\n",
    "    evalf = theano.function([X, y], [cost_eval, output_eval, test_acc], allow_input_downcast=True, name='eval')\n",
    "    \n",
    "    \n",
    "    return trainf, evalf, n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch tranining with mini-batch\n",
    "def epoch_func(X, y, fnc, penalty=False, batch_size=512):\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = int(np.ceil(num_samples / float(batch_size)))\n",
    "    costs = []\n",
    "    correct = 0\n",
    "    penalties = []\n",
    "    for i in range(num_batches):\n",
    "        idx = range(i*batch_size, np.minimum((i+1)*batch_size, num_samples))\n",
    "        X_batch = X[idx]\n",
    "        y_batch = y[idx]\n",
    "        outputs = fnc(X_batch, y_batch)\n",
    "        costs += [outputs[0]]\n",
    "        probs = outputs[1]\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        correct += np.sum(y_batch == preds)\n",
    "        #print(\"corr:\",correct)\n",
    "        if fnc.name.startswith('train'):\n",
    "            if penalty:\n",
    "                penalties += [outputs[2]]\n",
    "                \n",
    "    acc = correct/float(num_samples)\n",
    "    pen = []\n",
    "    if len(penalties)>0:\n",
    "        pen = np.mean(penalties)\n",
    "    \n",
    "    return np.mean(costs), acc , pen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENT SETTINGS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"bilisim/\"\n",
    "EXPERIMENT_ID = \"ex-2/\"\n",
    "RANDSEED = 10\n",
    "paper_fig_settings()\n",
    "\n",
    "from  datetime import datetime\n",
    "now = datetime.now()\n",
    "timestr = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir =  os.path.join(PROJECT_ROOT_DIR, \"outputs\", \n",
    "                        CHAPTER_ID+EXPERIMENT_ID)\n",
    "\n",
    "\n",
    "lasagne.random.set_rng(np.random.RandomState(RANDSEED))  # Set random state so we can investigate results\n",
    "np.random.seed(RANDSEED)\n",
    "np.set_printoptions(formatter={'float': '{: 0.4f}'.format}, suppress=True)\n",
    "\n",
    "PLOT_INT_RESULTS = False\n",
    "PRINT_INTERVAL = 10\n",
    "RECORD_INTERVAL= 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CHAPTER_ID+EXPERIMENT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config settingw\n",
    "Focusing_layer_params={}\n",
    "Focusing_layer_params['trainSis'] = True\n",
    "Focusing_layer_params['trainMus'] = True    \n",
    "Focusing_layer_params['trainWs'] = True\n",
    "Focusing_layer_params['withWeights'] = True\n",
    "Focusing_layer_params['initMu'] = 'spread'# 'middle' Or sth else\n",
    "Focusing_layer_params['initSigma'] = 0.1\n",
    "print(Focusing_layer_params)\n",
    "\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameter Test SETS\n",
    "NUM_EPOCHS = 500\n",
    "dset_choice = 'Synt' #'Mnist','Cifar10'\n",
    "#dset_choice = 'Mnist'\n",
    "Synthetic_Data_Params={}\n",
    "if dset_choice=='Synt':\n",
    "\n",
    "    Synthetic_Data_Params['nSAMPLES'] = 5000\n",
    "    Synthetic_Data_Params['nCLASSES'] = 4\n",
    "    #nFEATURES= np.array([4,10,40,100,1000])\n",
    "    Synthetic_Data_Params['nFEATURES']= 20\n",
    "    Synthetic_Data_Params['nCLUSTERS'] = 1\n",
    "    Synthetic_Data_Params['NOISE_DIMS'] = 0.5 # We add this many NOISE_DIMS*nFeatures to nFeatures\n",
    "    Synthetic_Data_Params['NOISE_SCALE'] = 1.0 # data is standard, this should not affect anything\n",
    "    Synthetic_Data_Params['NOISE_PATTERN']= 'sides' # position of the noise with respect to sampled dataset.\n",
    "\n",
    "Training_Params = {}\n",
    "Training_Params['BATCH_SIZE'] = 0.05 # portion of training data\n",
    "Training_Params['MOMENTUM'] = np.float32(0.9) # .9 is slightly worse\n",
    "Training_Params['LEARNING_RATE'] = np.float32(0.01) # works good. 0.001 is too low for many datasets. \n",
    "\n",
    "Network_Params = {}\n",
    "Network_Params['USEBATCHNORM'] = False #False is 2x 3x times faster, and performance is worse\n",
    "Network_Params['USE_PENALTY'] = False  # No significant difference perhaps due to low number of epochs\n",
    "Network_Params['nHIDDEN'] = 10\n",
    "\n",
    "\n",
    "# this method chooses random values from a list of arrays\n",
    "def choose_rand(x):\n",
    "    val=[]\n",
    "    for k in range(len(x)):\n",
    "        v = np.random.choice(x[k])\n",
    "        \n",
    "        val.append(v)\n",
    "    return val\n",
    "\n",
    "\n",
    "def set_lr_rates(d):\n",
    "    lr_all =d['LEARNING_RATE']    \n",
    "    lr_fw = lr_all\n",
    "    INIT_SIGMA = d['initSigma']\n",
    "    lr_tr_coeff = 39.08*np.exp(-20.65*INIT_SIGMA)+1.16\n",
    "    #lr_tr_coeff = 1.0\n",
    "    print(lr_tr_coeff)\n",
    "    lr_mu = lr_fw/lr_tr_coeff\n",
    "    lr_si = lr_mu*0.82\n",
    "    d['LR_MU']= np.float32(lr_mu)\n",
    "    d['LR_SI'] = np.float32(lr_si)\n",
    "    d['LR_FW'] = np.float32(lr_all)\n",
    "    \n",
    "# combine parameters\n",
    "\n",
    "pars ={}\n",
    "pars.update(Synthetic_Data_Params)\n",
    "pars.update(Training_Params)\n",
    "pars.update(Network_Params)\n",
    "pars.update(Focusing_layer_params)\n",
    "set_lr_rates(pars)\n",
    "\n",
    "\n",
    "print(pars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data set here. \n",
    "if dset_choice=='Synt':\n",
    "    \n",
    "    #set batch size so that it is reasonably smaller than nSAMPLES# larger batch sizes will require different LR\n",
    "    pars['BATCH_SIZE']=np.int(pars['BATCH_SIZE']*pars['nSAMPLES'])\n",
    "\n",
    "    # it is not possible to sample less dimensional data than nclusters*nclasses\n",
    "    pars['nFEATURES'] = max(pars['nFEATURES'],pars['nCLUSTERS']*pars['nCLASSES'])\n",
    "    if 2**pars['nFEATURES'] < pars['nCLUSTERS']*pars['nCLASSES']:\n",
    "        pars['nFEATURES'] = (pars['nCLUSTERS']*pars['nCLASSES'])+1\n",
    "    \n",
    "    print(\"Feature num check\", pars['nFEATURES']>=pars['nCLUSTERS']*pars['nCLASSES'])\n",
    "\n",
    "    noisy_features = int(pars['NOISE_DIMS']*pars['nFEATURES'])\n",
    "    \n",
    "    # at each repeat we create a train/test data first sample data \n",
    "    data = load_blob(classes=pars['nCLASSES'], features=pars['nFEATURES'], samples=pars['nSAMPLES'],\n",
    "                     random_state=RANDSEED, noise_dims=noisy_features,\n",
    "                     noise_scale=1.0, noise_pattern=pars['NOISE_PATTERN'],clusters=pars['nCLUSTERS'])\n",
    "\n",
    "    n_features = data['X_train'].shape[1]\n",
    "    X_train, y_train, X_test, y_test = data['X_train'], data['y_train'], data['X_test'],data['y_test']\n",
    "    X = T.fmatrix()\n",
    "    y = T.ivector('targets')\n",
    "    input_shape= (X_train.shape[0],X_train.shape[1])\n",
    "elif dset_choice=='Mnist':\n",
    "    data_root = '../datasets/image/'\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_dataset_mnist(folder=data_root)\n",
    "    n_features = X_train.shape[1] \n",
    "    pars['BATCH_SIZE'] = 512\n",
    "    pars['nCLASSES'] = 10\n",
    "    X = T.tensor4('inputs')\n",
    "    input_shape= (None, X_train.shape[1],X_train.shape[2],X_train.shape[3])\n",
    "    y = T.ivector('targets')\n",
    "    \n",
    "print(pars)\n",
    "print(\"Train data: \",X_train.shape,y_train.shape )\n",
    "print(\"Test data: \",X_test.shape,y_test.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dense, penalty_dense = build_dense_model(input_shape,X, pars)\n",
    "mp_dense = lasagne.layers.get_all_params(model_dense, trainable=True)\n",
    "\n",
    "\n",
    "model_focus, penalty_focus = build_focused_model(input_shape,X,  pars)\n",
    "mp_focus = lasagne.layers.get_all_params(model_focus, trainable=True)\n",
    "l_focused = next(l for l in lasagne.layers.get_all_layers(model_focus) if l.name is 'focus1')\n",
    "\n",
    "\n",
    "train_dense, eval_dense,n_netparamsd = build_functions(model_dense,X,y, pars,  penalty_dense)\n",
    "train_focus, eval_focus,n_netparamsf = build_functions(model_focus,X,y, pars, penalty_focus)\n",
    "pars['n_netparamsd'] = n_netparamsd\n",
    "pars['n_netparamsf'] = n_netparamsf\n",
    "\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create utility functions\n",
    "\n",
    "l_dense = next(l for l in lasagne.layers.get_all_layers(model_dense) if l.name is \"dense1\")\n",
    "dense_weights = l_dense.W\n",
    "\n",
    "l_focused = next(l for l in lasagne.layers.get_all_layers(model_focus) if l.name is 'focus1')\n",
    "\n",
    "# create functions to read mu, si, and weight values.\n",
    "get_si = lambda: l_focused.si.get_value()\n",
    "get_mu = lambda: l_focused.mu.get_value()\n",
    "get_foci = lambda: l_focused.calc_u().eval().T\n",
    "get_w = lambda: l_focused.W.get_value()\n",
    "set_w = lambda value: l_focused.W.set_value(value)\n",
    "\n",
    "\n",
    "# create output functions to read output of focus layer\n",
    "\n",
    "output_focus = lasagne.layers.get_output(l_focused, X, deterministic=True)\n",
    "eval_output_focus = theano.function([X], [output_focus], allow_input_downcast=True)\n",
    "    \n",
    "output_dense = lasagne.layers.get_output(l_dense, X, deterministic=True)\n",
    "eval_output_dense = theano.function([X], [output_dense], allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now do the training and test then record the outputs\n",
    "\n",
    "total_time_focus = 0\n",
    "total_time_dense = 0\n",
    "costs_dense, costs_focus = [], []\n",
    "costs_tst_dense, costs_tst_focus = [], []\n",
    "accs_dense, accs_focus = [], []\n",
    "\n",
    "focus_outputs = []\n",
    "dense_weights = []\n",
    "mus = []\n",
    "sis = []\n",
    "foci = []\n",
    "w_change = []\n",
    "scalers = []\n",
    "#np.random.shuffle(X_test)\n",
    "\n",
    "test_subset = X_test[np.random.permutation(X_test.shape[0])[0:500],:]\n",
    "try:\n",
    "    for n in range(NUM_EPOCHS):\n",
    "        \n",
    "        \n",
    "        if n == 0 or n%RECORD_INTERVAL==0: \n",
    "            mus.append(get_mu())\n",
    "            sis.append(get_si())\n",
    "            foci.append(get_foci()*get_w())\n",
    "            w_change.append(get_w())\n",
    "            \n",
    "            focus_output = eval_output_focus(test_subset)\n",
    "            focus_outputs.append(focus_output)\n",
    "            \n",
    "        #print(\"w:\",np.asarray(get_w())[:,0], \"mu: \",get_mu()[0],\"si: \",get_si()[0])\n",
    "        start_time = time.time()\n",
    "        train_cost_dense, train_acc_dense, penalty_dense = epoch_func(X_train, \n",
    "                                                                      y_train, \n",
    "                                                                      train_dense,\n",
    "                                                                      penalty=pars['USE_PENALTY'],\n",
    "                                                                      batch_size=pars['BATCH_SIZE'])\n",
    "        time_spent_dense = time.time() - start_time\n",
    "        total_time_dense +=time_spent_dense\n",
    "        \n",
    "        start_time = time.time()\n",
    "        train_cost_focus, train_acc_focus, penalty_focus = epoch_func(X_train, \n",
    "                                                                      y_train, \n",
    "                                                                      train_focus,\n",
    "                                                                      penalty=pars['USE_PENALTY'],\n",
    "                                                                      batch_size=pars['BATCH_SIZE'])\n",
    "        time_spent_focus = time.time() - start_time\n",
    "        total_time_focus +=time_spent_focus\n",
    "    \n",
    "        tst_cost_dense, acc_dense,_= epoch_func(X_test, y_test, eval_dense)\n",
    "        tst_cost_focus, acc_focus,_ = epoch_func(X_test, y_test, eval_focus)\n",
    "        \n",
    "        if not penalty_dense:\n",
    "            penalty_dense = 0 \n",
    "        if not penalty_focus:\n",
    "            penalty_focus = 0 \n",
    "        if np.mod(n, PRINT_INTERVAL) == 0:\n",
    "            print((\"Dense Ep {0}: Tcst {1:1.5f}, Tacc {2:1.3f}, \" +\n",
    "            \"Vcst {3:1.5f}, Vacc {4:3.3f}, Pen: {5:1.5f},\" +\n",
    "            \"Time: {6:1.4f}\").format(n, train_cost_dense, train_acc_dense,tst_cost_dense,acc_dense, penalty_dense, time_spent_dense))\n",
    "            print((\"Focus Ep {0}: Tcst {1:1.5f}, Tacc {2:1.3f}, \" +\n",
    "            \"Vcst {3:1.5f}, Vacc {4:3.3f}, Pen: {5:1.5f},\" +\n",
    "            \"Time: {6:1.4f}\").format(n, train_cost_focus, train_acc_focus,tst_cost_focus,acc_focus, penalty_focus,time_spent_focus))\n",
    "            \n",
    "            \n",
    "        costs_dense.append(train_cost_dense)\n",
    "        costs_focus.append(train_cost_focus)\n",
    "    \n",
    "        costs_tst_dense.append(tst_cost_dense)\n",
    "        costs_tst_focus.append(tst_cost_focus)\n",
    "    \n",
    "        accs_focus.append(acc_focus)\n",
    "        accs_dense.append(acc_dense)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "acc_focus_np = np.array(accs_focus)\n",
    "acc_dense_np = np.array(accs_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import save_fig, paper_fig_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 22\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=SMALL_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.title(u\"Loss\")\n",
    "plt.plot(np.array(costs_focus), linestyle='--',color='r', linewidth=2)\n",
    "plt.plot(np.array(costs_dense), linestyle='--', color='g', linewidth=2)\n",
    "plt.plot(np.array(costs_tst_focus), color='r', linewidth=4)\n",
    "plt.plot(np.array(costs_tst_dense), color='g', linewidth=4)\n",
    "\n",
    "#plt.yscale('log')\n",
    "#plt.grid(True, which='both')\n",
    "plt.legend([u'Fcs-trn', u'Dns-trn', u'Fcs-tst',u'Dns-tst'])\n",
    "plt.grid(True)\n",
    "#plt.xlabel('Epoch')\n",
    "plt.xlabel('Epok')\n",
    "#plt.ylabel('Error')\n",
    "plt.ylabel('Hata')\n",
    "plt.xlim(0,len(costs_focus))\n",
    "#save_fig('focus_train')\n",
    "save_fig('training_val_loss_synthetic', postfix=CHAPTER_ID+EXPERIMENT_ID)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_focus_np = np.array(accs_focus)\n",
    "acc_dense_np = np.array(accs_dense)\n",
    "plt.figure(figsize=(6,6))\n",
    "#plt.title(u'Doğrulama Kümesi Doğruluk')\n",
    "plt.plot(np.array(acc_focus_np), color='r', linewidth=3, alpha=0.5)\n",
    "plt.plot(np.array(acc_dense_np), color='g', linewidth=3, alpha=0.5)\n",
    "#plt.legend([u'Odaklanan Nöron Katmanlı Sinir Ağı', u'Tümden Bağlı Katmanli Yapay Sinir Ağı'])\n",
    "plt.legend([u'Focused', u'Dense'])\n",
    "#save_fig('focus_acc')\n",
    "plt.xlabel('Epok')\n",
    "plt.ylabel(u'Sınıflandırma Doğruluğu (Acc)')\n",
    "plt.grid(True)\n",
    "plt.ylim([0.5,1.0])\n",
    "save_fig('val_accuracy_synth',postfix=CHAPTER_ID+EXPERIMENT_ID)\n",
    "plt.show()\n",
    "#savefig(fname, dpi=None, facecolor='w', edgecolor='w',\n",
    "#          orientation='portrait', papertype=None, format=None,\n",
    "#          transparent=False, bbox_inches=None, pad_inches=0.1,\n",
    "#          frameon=None, metadata=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_outputs_np = np.array(focus_output).reshape((-1, pars['nHIDDEN']))\n",
    "for i in range(3):\n",
    "    x = focus_outputs_np[:, i]\n",
    "    n, bins, patches = plt.hist(x, 50, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "    plt.xlabel('Output Activation Distribution')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(r'$\\mathrm{Histogram\\ of\\ focus\\ Neuron:}\\ '+ str(i) +'$')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "markers = itertools.cycle((',', '+', '.', '^', '*')) \n",
    "styler = itertools.cycle(('-', '--', '-.')) \n",
    "\n",
    "mu = np.array(mus)\n",
    "print (mu.shape)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.title(r'Foci center change')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(r'$\\mu$')\n",
    "plt.ylim([0.2,0.8])\n",
    "\n",
    "#plt.title(r'$\\mu$' + u' Değisimi')\n",
    "#plt.xlabel('Epok')\n",
    "#plt.ylabel(r'$\\mu$')\n",
    "\n",
    "plt.plot(mu[:, 0] , marker='.')\n",
    "for x in range(mu.shape[1]):\n",
    "    plt.plot(mu[0:-1, x], marker='.')\n",
    "plt.grid(True)\n",
    "#plt.xlim((-100,NUM_EPOCHS))\n",
    "#plt.xscale('log')\n",
    "save_fig('mu_change',postfix=CHAPTER_ID+EXPERIMENT_ID)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = np.array(sis)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.title(r'$\\sigma$' + u' Change')\n",
    "\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(r'$\\sigma$')\n",
    "\n",
    "#plt.plot(mu[:, 0] * 16, marker='o')\n",
    "for x in range(si.shape[1]):\n",
    "    plt.plot(si[0:-1, x], marker='.')\n",
    "plt.grid(True)\n",
    "#plt.xlim((-100,NUM_EPOCHS))\n",
    "#plt.xscale('log')\n",
    "save_fig('si_change', postfix=CHAPTER_ID+EXPERIMENT_ID)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
